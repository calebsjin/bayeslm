[
["intro.html", "Chapter 1 Introduction 1.1 Linear model 1.2 Maximum likelihood estimation", " Chapter 1 Introduction 1.1 Linear model Consider a multiple linear regression model as follows: \\[{\\bf y}={\\bf X}{\\boldsymbol \\beta}+ {\\boldsymbol \\epsilon},\\] where \\({\\bf y}=(y_1,y_2,\\dots,y_n)^{{\\bf T}}\\) is the \\(n\\)-dimensional response vector, \\({\\bf X}=({\\bf x}_1,{\\bf x}_2, \\dots, {\\bf x}_n)^{{\\bf T}}\\) is the \\(n\\times p\\) design matrix, and \\({\\boldsymbol \\epsilon}\\sim \\mathcal{N}_n({\\boldsymbol 0},\\sigma^2{\\bf I}_n)\\). We assume that \\(p&lt;n\\) and \\({\\bf X}\\) is full rank. 1.2 Maximum likelihood estimation Since \\({\\bf y}\\sim \\mathcal{N}_n({\\bf X}{\\boldsymbol \\beta}, \\sigma^2{\\bf I}_n)\\), the likelihood function is given as \\[\\begin{eqnarray*} L({\\boldsymbol \\beta},\\sigma^2)&amp;=&amp;f({\\bf y}|{\\boldsymbol \\beta},\\sigma^2)\\\\ &amp;= &amp;(2\\pi)^{-\\frac{n}{2}}\\lvert {\\boldsymbol \\Sigma}\\lvert^{-\\frac{1}{2}} \\exp\\left\\{-\\frac{1}{2}({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^{{\\bf T}}{\\boldsymbol \\Sigma}^{-1}({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\right\\} , \\end{eqnarray*}\\] where \\({\\boldsymbol \\Sigma}=\\sigma^2 {\\bf I}_n\\). Then the log likelihood can be written as \\[\\begin{eqnarray*} l({\\boldsymbol \\beta},\\sigma^2)&amp;=&amp;\\log L({\\boldsymbol \\beta},\\sigma^2)\\\\ &amp;=&amp;-\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^{{\\bf T}}({\\bf y}-{\\bf X}{\\boldsymbol \\beta}). \\end{eqnarray*}\\] Note that \\(l({\\boldsymbol \\beta},\\sigma^2)\\) is a concave function in \\(({\\boldsymbol \\beta},\\sigma^2)\\). Hence, the maximum likelihood estimator (MLE) can be obtained by solving the following equations: \\[\\begin{eqnarray*} \\frac{\\partial l({\\boldsymbol \\beta},\\sigma^2)}{\\partial {\\boldsymbol \\beta}} &amp;=&amp;- \\frac{1}{2\\sigma^2}(-2{\\bf X}^{{\\bf T}} {\\bf y}+ 2{\\bf X}^{{\\bf T}} {\\bf X}{\\boldsymbol \\beta})=0; \\frac{\\partial l({\\boldsymbol \\beta},\\sigma^2)}{\\partial \\sigma^2} &amp;=&amp;-\\frac{n}{2}\\frac{1}{\\sigma^2} + \\frac{1}{2}\\frac{1}{(\\sigma^2)^2} \\mathbf{({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^T({\\bf y}-{\\bf X}{\\boldsymbol \\beta})}=0. \\end{eqnarray*}\\] Therefore, the MLEs of \\({\\boldsymbol \\beta}\\) and \\(\\sigma^2\\) are given as \\[\\begin{eqnarray*} \\hat{\\beta} &amp;=&amp; ({\\bf X}^{{\\bf T}} {\\bf X})^{-1}{\\bf X}^{{\\bf T}} {\\bf y};\\\\ \\hat{\\sigma}^2 &amp;=&amp; \\frac{({\\bf y}-{\\bf X}\\hat{{\\boldsymbol \\beta}})^{{\\bf T}}({\\bf y}-{\\bf X}\\hat{{\\boldsymbol \\beta}})}{n}=\\frac{ \\|{\\bf y}-\\hat{{\\bf y}}\\|^2}{n}, \\end{eqnarray*}\\] where \\(\\hat{{\\bf y}}={\\bf X}\\hat{{\\boldsymbol \\beta}}\\). Note that if \\({\\bf z}\\sim \\mathcal{N}_k(\\mu,\\Sigma)\\), then \\(A{\\bf z}\\sim \\mathcal{N}_m(A\\mu,A\\Sigma A^{{\\bf T}})\\), where \\(A\\) is an \\(m\\times k\\) matrix. Since \\({\\bf y}\\sim \\mathcal{N}_n({\\bf X}{\\boldsymbol \\beta}, \\sigma^2{\\bf I}_n)\\) and \\(\\hat{{\\boldsymbol \\beta}} =({\\bf X}^{{\\bf T}} {\\bf X})^{-1}{\\bf X}^{{\\bf T}}{\\bf y}\\), we have \\[\\begin{eqnarray*} \\hat{{\\boldsymbol \\beta}} &amp;\\sim&amp; \\mathcal{N}_p\\left( ({\\bf X}^{{\\bf T}} {\\bf X})^{-1}{\\bf X}^{{\\bf T}}{\\bf X}{\\boldsymbol \\beta}, \\sigma^2 ({\\bf X}^{{\\bf T}} {\\bf X})^{-1}{\\bf X}^{{\\bf T}}{\\bf X}({\\bf X}^{{\\bf T}} {\\bf X})^{-1}\\right)\\\\ &amp;=&amp;\\mathcal{N}_p\\left({\\boldsymbol \\beta}, \\sigma^2 ({\\bf X}^{{\\bf T}} {\\bf X})^{-1}\\right). \\end{eqnarray*}\\] Note that \\({\\bf y}-\\hat{{\\bf y}}=({\\bf I}_n-{\\bf X}({\\bf X}^{{\\bf T}}{\\bf X})^{-1}{\\bf X}^{{\\bf T}}){\\bf y}\\), where \\({\\bf I}_n-{\\bf X}({\\bf X}^{{\\bf T}}{\\bf X})^{-1}{\\bf X}^{{\\bf T}}\\) is an idempotent matrix of rank \\((n-p)\\). \\(\\color{red}{\\text{Can you prove that ${\\bf I}_n-{\\bf X}({\\bf X}^{{\\bf T}}{\\bf X})^{-1}{\\bf X}^{{\\bf T}}$ s an idempotent matrix of rank $(n-p)$ ?}}\\) Proof: Let \\({\\bf H}= {\\bf X}({\\bf X}^{{\\bf T}}{\\bf X})^{-1}{\\bf X}^{{\\bf T}}\\). \\({\\bf H}{\\bf H}= {\\bf X}({\\bf X}^{{\\bf T}}{\\bf X})^{-1}{\\bf X}^{{\\bf T}}{\\bf X}({\\bf X}^{{\\bf T}}{\\bf X})^{-1}{\\bf X}^{{\\bf T}}={\\bf X}({\\bf X}^{{\\bf T}}{\\bf X})^{-1}{\\bf X}^{{\\bf T}}={\\bf H}\\) ,hence \\({\\bf H}\\) is idempotent matrix. Then \\(({\\bf I}_n-{\\bf H})({\\bf I}_n-{\\bf H}) = {\\bf I}_n-{\\bf H}\\), hence \\(({\\bf I}_n-{\\bf H})\\) is also idempotent. hence we have \\(r({\\bf I}_n-{\\bf H})=tr({\\bf I}_n-{\\bf H})=n-tr({\\bf H})=n-tr(({\\bf X}^{{\\bf T}}{\\bf X})^{-1}{\\bf X}^{{\\bf T}}{\\bf X})=n-p\\) \\(\\color{red}{\\text{How to prove $r({\\bf I}_n-{\\bf H})=tr({\\bf I}_n-{\\bf H})$? }}\\) The eigenvalue of idempotent matrix is either 1 or 0, hence the rank of it is the sum of eigenvalues, which equals the trace of matrix; \\(\\color{red}{\\text{How to prove trace of matrix is the sum of eigenvalues?}}\\) It requires characteristic polynomial. Another way to prove this is in this link From Lemma , we have that \\[\\begin{eqnarray} \\label{eq:1} n\\frac{\\hat{\\sigma}^2}{\\sigma^2}\\sim \\chi^2(n-p), \\end{eqnarray}\\] where \\(\\chi^2(n-p)\\) denotes the chi-squared distribution with degrees of freedom \\(n-p\\). Proof: By Lemma 1, we have \\\\(n\\frac{\\hat{\\sigma}^2}{\\sigma^2}=\\frac{\\hat e^{{\\bf T}}\\hat e}{\\sigma^2}={\\bf y}^{{\\bf T}}(\\frac{{\\bf I}_n-{\\bf H}}{\\sigma^2}) {\\bf y}={\\bf y}^{{\\bf T}}{\\bf A}{\\bf y}\\sim\\chi^2(tr({\\bf A}{\\boldsymbol \\Sigma}),\\mu^{{\\bf T}}{\\bf A}\\mu/2)\\) where \\({\\bf A}=\\frac{{\\bf I}_n-{\\bf H}}{\\sigma^2}\\), \\(\\mu^{{\\bf T}}{\\bf A}\\mu/2=({\\bf X}{\\boldsymbol \\beta})^{{\\bf T}}(\\frac{{\\bf I}_n-{\\bf H}}{\\sigma^2})({\\bf X}{\\boldsymbol \\beta})/2=0\\). \\({\\bf A}{\\boldsymbol \\Sigma}= (\\frac{{\\bf I}_n-{\\bf H}}{\\sigma^2})\\sigma^2{\\bf I}_n={\\bf I}_n-{\\bf H},\\) hence \\(r({\\bf A}{\\boldsymbol \\Sigma}) = n-p\\). Therefore \\(n\\frac{\\hat{\\sigma}^2}{\\sigma^2}\\sim \\chi^2(n-p)\\) "]
]
